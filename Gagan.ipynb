{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current branch name: \n",
      "3D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soumensardar/miniconda3/envs/tf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "print(\"Current branch name: \")\n",
    "!git branch --show-current\n",
    "\n",
    "try:\n",
    "    from rich import print\n",
    "except:\n",
    "    !pip install rich\n",
    "    from rich import print\n",
    "\n",
    "try:\n",
    "    import tensorflow_datasets, tensorflow, matplotlib, numpy, PIL, sklearn\n",
    "except:\n",
    "    !pip install tensorflow_datasets tensorflow matplotlib numpy pillow scikit-learn\n",
    "    import tensorflow_datasets, tensorflow, matplotlib, numpy, PIL, sklearn\n",
    "\n",
    "try:\n",
    "    import keras_tuner as kt\n",
    "except:\n",
    "    !pip install keras-tuner\n",
    "    import keras_tuner as kt\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    !pip install opencv-python\n",
    "    import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "try:\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import torchvision.transforms\n",
    "except:\n",
    "    !pip install torch torchvision\n",
    "    import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT TOUCH BELOW ==================================\n",
    "FULL_DATASET = 0\n",
    "FULL_BINARY_DATASET = 1\n",
    "MY_DATASET = 2\n",
    "MY_BIN_DATASET = 3\n",
    "\n",
    "# DO NOT TOUCH - END ===================================\n",
    "\n",
    "# select dataset\n",
    "SELECTED_DATASET = MY_DATASET # CHANGE HERE\n",
    "\n",
    "\n",
    "\n",
    "# DO NOT TOUCH BELOW ==================================\n",
    "FULL_DATASET_PATH = r'D:\\CGC work\\Alziehmer Disease\\PhdNotebook\\OASIS dataset\\OASIS dataset\\Data'\n",
    "FULL_BINARY_DATASET_PATH = r'D:\\CGC work\\Alziehmer Disease\\PhdNotebook\\OASIS dataset Binary\\OASIS dataset\\Data'\n",
    "MY_DATASET_PATH = r'/Users/soumensardar/Downloads/OASIS/'\n",
    "MY_DATASET_BINARY_PATH = r'/Users/soumensardar/Downloads/OASIS-binary/'\n",
    "\n",
    "# while adding more datasets, make sure to add tflog directory\n",
    "datasetdir, dataset_name =[  (FULL_DATASET_PATH, 'tflogs_3d'),\n",
    "                             (FULL_BINARY_DATASET_PATH, 'tflogs_binary_3d'),\n",
    "                             (MY_DATASET_PATH, 'tflogs_my_3d'),\n",
    "                             (MY_DATASET_BINARY_PATH, 'tflogs_mybin_3d'),\n",
    "                    ][SELECTED_DATASET]\n",
    "# DO NOT TOUCH - END ===================================\n",
    "datasetdir, dataset_name\n",
    "assert os.path.exists(datasetdir), f\"Dataset path is incorrect {datasetdir}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Image selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/soumensardar/Downloads/OASIS/Mild Dementia/OAS1_0028_MR1_mpr-1_127.jpg',\n",
       " 'Mild Dementia')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_IMG_PATH, TEST_IMG_LABEL = [\n",
    "(r'D:\\CGC work\\Alziehmer Disease\\PhdNotebook\\OASIS dataset\\OASIS dataset\\Data\\Mild Dementia/OAS1_0028_MR1_mpr-1_127.jpg', 'Mild_Dementia'),\n",
    "(r'D:\\CGC work\\Alziehmer Disease\\PhdNotebook\\OASIS dataset Binary\\OASIS dataset\\Data\\1. Mild Dementia/OAS1_0028_MR1_mpr-1_127.jpg', '1. Mild_Dementia'),\n",
    "(r'/Users/soumensardar/Downloads/OASIS/Mild Dementia/OAS1_0028_MR1_mpr-1_127.jpg', 'Mild Dementia'),\n",
    "(r'/Users/soumensardar/Downloads/OASIS-binary/Mild Dementia/OAS1_0028_MR1_mpr-1_127.jpg', '1. Mild Dementia'),\n",
    "][SELECTED_DATASET]\n",
    "import os\n",
    "assert os.path.exists(TEST_IMG_PATH), f\"Test image path is incorrect {TEST_IMG_PATH}\"\n",
    "TEST_IMG_PATH, TEST_IMG_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "def get_experiment_details(dataset, model, ts):\n",
    "    exp_asset_dir = f\"results/{dataset}/{model}/{ts}\"\n",
    "    assert os.path.exists(exp_asset_dir), f\"Experiment does not exist '{exp_asset_dir}'\"\n",
    "    checkpoint_model_dir = None\n",
    "    with open(exp_asset_dir + \"/currentepoch.txt\") as fp:\n",
    "        last_epoch = int(fp.read().strip())\n",
    "        checkpoint_model_dir = exp_asset_dir + \"/models/epoch=%02d.h5\"%last_epoch\n",
    "        assert os.path.exists(checkpoint_model_dir), f\"Unable to find the last checkpoint file {checkpoint_model_dir}\"\n",
    "    best_model_dir = exp_asset_dir + \"/models/best-model.h5\"\n",
    "    if not os.path.exists(best_model_dir):\n",
    "        best_model_dir = None\n",
    "\n",
    "    best_model_info = None\n",
    "    if os.path.exists(exp_asset_dir + \"/bestvalue.json\"):\n",
    "        with open(exp_asset_dir + \"/bestvalue.json\") as fp:\n",
    "            best_model_info = json.load(fp)\n",
    "        \n",
    "    return dict(last_epoch=last_epoch, \n",
    "                best_checkpoint=best_model_dir, \n",
    "                last_checkpoint=checkpoint_model_dir,\n",
    "                best_model_info=best_model_info,\n",
    "                project_dir=exp_asset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">what_changed</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Training with model_name='3DMobileNet' task_type='categorical' lr_schedular=False </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">activation='softmax' loss_function='categorical_crossentropy'\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mwhat_changed\u001b[0m=\u001b[32m\"Training\u001b[0m\u001b[32m with \u001b[0m\u001b[32mmodel_name\u001b[0m\u001b[32m='3DMobileNet' \u001b[0m\u001b[32mtask_type\u001b[0m\u001b[32m='categorical' \u001b[0m\u001b[32mlr_schedular\u001b[0m\u001b[32m=\u001b[0m\u001b[32mFalse\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mactivation\u001b[0m\u001b[32m='softmax' \u001b[0m\u001b[32mloss_function\u001b[0m\u001b[32m='categorical_crossentropy'\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_seed=37\n",
    "resume_training_timestamp = None # CHANGE HERE\n",
    "max_epoch = 50 # CHANGE HERE\n",
    "batch_size = 64 # CHANGE HERE\n",
    "image_size = 128\n",
    "color_mode = 'rgb'\n",
    "n_ch = dict(rgb=3, grayscale=1)[color_mode]\n",
    "monitor = \"f1_score\"\n",
    "initial_threshold = 0.5\n",
    "mode=\"max\"\n",
    "freq=\"epoch\"\n",
    "initial_epoch=0\n",
    "\n",
    "learning_rate = 0.001\n",
    "lr_schedular = False\n",
    "\n",
    "enable_class_weight = False\n",
    "ablation_study_size = 0 # batch_size * 30 # CHANGE HERE\n",
    "\n",
    "task_type = \"categorical\" # \"binary\"\n",
    "# Link: https://www.tensorflow.org/api_docs/python/tf/keras/losses#functions\n",
    "if task_type == \"binary\":\n",
    "    activation = \"sigmoid\"\n",
    "    loss_function = \"binary_crossentropy\"\n",
    "else:\n",
    "    activation = \"softmax\"\n",
    "    loss_function = \"categorical_crossentropy\"\n",
    "\n",
    "resume_checkpoint_path = None\n",
    "# Link: https://keras.io/api/applications/\n",
    "model_name=\"3DMobileNet\" # CHANGE HERE !!!!!!!\n",
    "# add a comment about what changes you have done just now before running the training\n",
    "what_changed = f\"Training with {model_name=} {task_type=} {lr_schedular=} {activation=} {loss_function=}\"\n",
    "print(f\"{what_changed=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "2024-12-21 00:45:38.591455: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-12-21 00:45:38.591477: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-21 00:45:38.591483: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-21 00:45:38.591672: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-21 00:45:38.591840: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# EACH LINK CONTAINS AVAILABLE OPTIONS\n",
    "# Link: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers#classes\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate) # CHANGE HERE\n",
    "# Link: https://www.tensorflow.org/api_docs/python/tf/keras/metrics#classes\n",
    "metrics = [tf.keras.metrics.Accuracy(), \n",
    "           tf.keras.metrics.Recall(), \n",
    "           tf.keras.metrics.Precision(),\n",
    "           tf.keras.metrics.F1Score(average=\"macro\"),\n",
    "           tf.keras.metrics.SensitivityAtSpecificity(0.6),\n",
    "           tf.keras.metrics.SpecificityAtSensitivity(0.6),\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedular callback\n",
    "def lr_schedule(epoch):\n",
    "  \"\"\"\n",
    "  Returns a custom learning rate that decreases as epochs progress.\n",
    "  \"\"\"\n",
    "  learning_rate = 0.02\n",
    "  if epoch > 5:\n",
    "    learning_rate = 0.01\n",
    "  if epoch > 10:\n",
    "    learning_rate = 0.0001\n",
    "  if epoch > 15:\n",
    "    learning_rate = 0.00001\n",
    "\n",
    "  tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "  return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_training_timestamp:\n",
    "    print(f\"Trying to resume from checkpoint... {resume_training_timestamp}\")\n",
    "    d = get_experiment_details(dataset_name, model_name, resume_training_timestamp)\n",
    "    initial_epoch = d['last_epoch']\n",
    "    resume_checkpoint_path = d['last_checkpoint']\n",
    "    assert os.path.exists(resume_checkpoint_path), f\"Unable to resume training from '{d['project_dir']}'.\"\n",
    "    best_model_info = d['best_model_info']\n",
    "    if best_model_info:\n",
    "        print(\"Updating the metric monitoring parameters before resuming the checkpoint\")\n",
    "        monitor = best_model_info['monitor']\n",
    "        initial_threshold = best_model_info['value']\n",
    "        mode=best_model_info['mode']\n",
    "        freq=best_model_info['frequency']\n",
    "    print(f\"Resuming checkpoint form epoch={initial_epoch}.\")\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "def save_hparams():\n",
    "    hyprams = dict(\n",
    "            timestamp = timestamp,\n",
    "            what_changed=what_changed,\n",
    "            random_state=random_seed,\n",
    "            max_epoch=max_epoch,\n",
    "            batch_size=batch_size,\n",
    "            image_size=image_size,\n",
    "            num_channels=n_ch,\n",
    "            metrics=str(metrics),\n",
    "            loss_function=loss_function,\n",
    "            monitoring=dict(monitor = monitor, initial_threshold = initial_threshold, mode=mode),\n",
    "            resume_training_timestamp=resume_training_timestamp,\n",
    "            initial_epoch=initial_epoch,\n",
    "            model_name=model_name,\n",
    "            optimizer=optimizer.name,\n",
    "            learning_rate=learning_rate,\n",
    "            lr_schedular=lr_schedular,\n",
    "        )\n",
    "    import json \n",
    "    log(\"Saving hyperparameters.\")\n",
    "    print(hyprams)\n",
    "    # Convert and write JSON object to file\n",
    "    with open(f\"{artifact_root}/hyperparams.json\", \"w\") as outfile: \n",
    "        json.dump(hyprams, outfile, indent=4)\n",
    "    file_writer = tf.summary.create_file_writer(tf_log_dir + \"/hparams\")\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.text(\"hyperparams.json\", f\"{artifact_root}/hyperparams.json\", step=0)\n",
    "        for k, v in hyprams.items():\n",
    "            if isinstance(v, int):\n",
    "                tf.summary.scalar(k, v, step=0)\n",
    "            elif isinstance(v, float):\n",
    "                tf.summary.scalar(k, v, step=0)\n",
    "            else:\n",
    "                tf.summary.text(k, str(v), step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare artifact directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Experiment path: <span style=\"color: #008000; text-decoration-color: #008000\">'results/tflogs_my_3d/3DMobileNet/20241221-004538'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Experiment path: \u001b[32m'results/tflogs_my_3d/3DMobileNet/20241221-004538'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Experiment path: <span style=\"color: #008000; text-decoration-color: #008000\">'results/tflogs_my_3d/3DMobileNet/20241221-004538'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Experiment path: \u001b[32m'results/tflogs_my_3d/3DMobileNet/20241221-004538'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if resume_training_timestamp:\n",
    "    print(\"Resume timestamp\", resume_training_timestamp)\n",
    "    timestamp = resume_training_timestamp\n",
    "unique_dir = f\"{model_name}/{timestamp}\"\n",
    "tf_log_dir = f\"results/{dataset_name}/{unique_dir}\"\n",
    "tf_log_img_dir = f\"results/{dataset_name}/images\"\n",
    "artifact_root  = f\"results/{dataset_name}/{unique_dir}\"\n",
    "pathlib.Path(artifact_root).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(tf_log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def log(*args, **kwargs):\n",
    "    time = False\n",
    "    if \"time\" in kwargs.keys():\n",
    "        time = kwargs[\"time\"]\n",
    "        del kwargs[\"time\"]\n",
    "    if time:\n",
    "        time = datetime.datetime.now().strftime(\"%H:%M:%S %d/%m/%Y\")\n",
    "        a = list(args)\n",
    "        a.append(time)\n",
    "        args = tuple(a)\n",
    "    print(*args, **kwargs)\n",
    "    with open(f\"{artifact_root}/additional_logs.txt\", \"a\") as fp:\n",
    "        kwargs[\"file\"] = fp\n",
    "        kwargs[\"flush\"] = True\n",
    "        print(*args, **kwargs)\n",
    "log(f\"Experiment path: '{artifact_root}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Training Resume Timestamp\n",
    "## Use this `timestamp` to update `resume_training_timestamp` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Resume timestamp: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20241221</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">004538</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Resume timestamp: \u001b[1;36m20241221\u001b[0m-\u001b[1;36m004538\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Resume timestamp: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20241221</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">004538</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Resume timestamp: \u001b[1;36m20241221\u001b[0m-\u001b[1;36m004538\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log(f\"Resume timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D6HlWjJDFnvy"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
    "#   DataGenerator to read images and rescale images\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import random\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading 3D datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class OASISTorchDataset(Dataset):\n",
    "    BINARY_TASK = \"binary\"\n",
    "    CATEGORY_TASK = \"categorical\"\n",
    "    VOLUME_DEPTH = 61\n",
    "\n",
    "    def __init__(self, path, task_type, image_size, transforms, nch=1, seed=37, class_names=None, dtype=\"float32\"):\n",
    "        self.path = pathlib.Path(path)\n",
    "        self.classnames = [i for i in os.listdir(path)]\n",
    "        assert len(image_size) == 2, \"Image size must be a tuple of two integers\"\n",
    "        self.image_size = image_size\n",
    "        self.dtype = dtype\n",
    "        self.transforms = transforms\n",
    "        if class_names is not None:\n",
    "            self.classnames = [c for c in self.classnames if c in class_names]\n",
    "        self.num_classes = len(self.classnames)\n",
    "        if task_type == self.BINARY_TASK:\n",
    "            self.num_classes = 1\n",
    "        self.task_type = task_type\n",
    "        assert nch in (1, 3), \"Supported channels are 1(greyscale) and 3(rgb)\"\n",
    "        self.nch = nch\n",
    "        if self.task_type == self.BINARY_TASK and len(self.classnames) >= 2 and class_names is None:\n",
    "            raise Exception(f\"When `class_names` is not specified, \"\n",
    "                            f\"only 2 classes are allowed in the data-directory, but {len(self.classnames)} found.\")\n",
    "        self.items = []\n",
    "        self._load(seed)\n",
    "\n",
    "    @staticmethod\n",
    "    def _group_images_by_subject(image_dir):\n",
    "        \"\"\"Groups images by subject ID based on their filenames.\n",
    "        Args:\n",
    "            image_dir: Path to the directory containing the images.\n",
    "        Returns:\n",
    "            A dictionary where keys are subject IDs and values are lists of image filenames.\n",
    "        \"\"\"\n",
    "        image_files = os.listdir(image_dir)\n",
    "        subject_groups = {}\n",
    "        for filename in image_files:\n",
    "            # Extract subject ID from the filename (adjust the pattern as needed)\n",
    "            subject_id = \"_\".join(filename.split('_')[0:-1])\n",
    "            if subject_id not in subject_groups:\n",
    "                subject_groups[subject_id] = []\n",
    "            subject_groups[subject_id].append(os.path.join(image_dir, filename))\n",
    "        print(\"For\", len(subject_groups.keys()), \"patients,\", len(image_files), f\"images scanned from '{image_dir}'\")\n",
    "        return subject_groups\n",
    "\n",
    "    def item_generator(self):\n",
    "        \"\"\"\n",
    "        :return: yield MRI slices as list and encoded label\n",
    "        \"\"\"\n",
    "        for class_dir in self.classnames:\n",
    "            grouped_files = self._group_images_by_subject((self.path / class_dir).resolve())\n",
    "            for slices, label in zip(grouped_files.values(), [class_dir] * len(grouped_files.values())):\n",
    "                lbl = self._encode_labels(label)\n",
    "                yield slices, lbl\n",
    "\n",
    "    def volume_generator(self):\n",
    "        \"\"\"\n",
    "        :return: yield MRI slices as 3D RGB image volume and encoded label\n",
    "        \"\"\"\n",
    "        for class_dir in self.classnames:\n",
    "            grouped_files = self._group_images_by_subject((self.path / class_dir).resolve())\n",
    "            for slices, label in zip(grouped_files.values(), [class_dir] * len(grouped_files.values())):\n",
    "                vol = self._load_volume(slices)\n",
    "                lbl = self._encode_labels(label)\n",
    "                yield vol, lbl\n",
    "\n",
    "    def _encode_labels(self, labels):\n",
    "        \"\"\"\n",
    "        :param labels: this could be a string or list of strings\n",
    "        :return: encoded binary tensor of [1,0,1,...] or [[0,1],[1,0],[0,1],...]\n",
    "                for categorical\n",
    "        \"\"\"\n",
    "        is_one_element = False\n",
    "        if isinstance(labels, str):\n",
    "            labels = [labels]\n",
    "            is_one_element = True\n",
    "        enc_label = []\n",
    "        for lbl in labels:\n",
    "            if lbl not in self.classnames:\n",
    "                warnings.warn(f\"'{lbl}' label is unknown\")\n",
    "            if self.task_type == self.BINARY_TASK:\n",
    "                enc_label.append(self.classnames.index(lbl))\n",
    "            else:\n",
    "                enc_label.append([lbl == cls_nm for cls_nm in self.classnames])\n",
    "        if is_one_element:\n",
    "            return np.array(enc_label[0]).astype(self.dtype)\n",
    "        else:\n",
    "            return np.array(enc_label).astype(self.dtype)\n",
    "\n",
    "    def _decode_labels(self, probabilities, probability_threshold=0.5):\n",
    "        \"\"\"\n",
    "        :param probabilities: batch of the network output(probabilities)\n",
    "        :param probability_threshold: to decide the class\n",
    "        :return: decode [probas1, probas2, ...] to tensor of [cls1, cls2,...]\n",
    "        \"\"\"\n",
    "        assert len(probabilities.shape) == 2, f\"`probabilities` shape must be two dimensional\"\n",
    "        if self.task_type == self.BINARY_TASK:\n",
    "            assert probabilities.shape[1] == 1, f\"`probabilities` shape must be (batch_size, 1)\"\n",
    "            binaries = probabilities >= probability_threshold\n",
    "            return list(map(lambda oi: self.classnames[int(oi)], binaries))\n",
    "        else:\n",
    "            assert probabilities.shape[1] == len(self.classnames), (f\"`probabilities` shape must be\"\n",
    "                                                                    f\" (batch_size, num_classes)\")\n",
    "            probability_threshold = [probability_threshold] * len(self.classnames)\n",
    "            indices = (probabilities >= probability_threshold).numpy().argmax(axis=1)\n",
    "            string_array = np.array(self.classnames)\n",
    "            return string_array[indices].tolist()\n",
    "\n",
    "    def _load_volume(self, image_files: list, label=None):\n",
    "        \"\"\"Loads a 3D volume from a directory of JPEG images.\n",
    "        Args:\n",
    "            image_files: list of files\n",
    "        Returns:\n",
    "            A 3D representing the 3D volume\n",
    "        \"\"\"\n",
    "        image_volume = []\n",
    "        for image_path in image_files:\n",
    "            image_data = cv2.imread(image_path)\n",
    "            if self.nch == 1:\n",
    "                image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2GRAY)\n",
    "            else:\n",
    "                image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)\n",
    "            # resize\n",
    "            image_data = cv2.resize(image_data, self.image_size)\n",
    "            # transforming images\n",
    "            image_data = self.transforms(image_data.astype(\"uint8\"))\n",
    "            # creating volume\n",
    "            image_volume.append(image_data)\n",
    "        volume = np.stack(image_volume, axis=0).astype(self.dtype)\n",
    "        if label is None:\n",
    "            return volume\n",
    "        else:\n",
    "            return volume, label\n",
    "\n",
    "    def _load(self, seed):\n",
    "        random.seed(seed)\n",
    "        self.items = list(self.item_generator())\n",
    "        random.shuffle(self.items)\n",
    "        return self\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._load_volume(*self.items[idx])\n",
    "\n",
    "\n",
    "def torch_to_tf(torch_dataset, task_type, image_size, nch=3, dtype=\"float32\"):\n",
    "    def torch_data_gen():\n",
    "        for img, lbl in torch_dataset:\n",
    "            yield tf.convert_to_tensor(img), tf.convert_to_tensor(lbl)\n",
    "\n",
    "    input_shape = (61, *image_size, nch)\n",
    "    label_shape = (1,) if task_type == OASISTorchDataset.BINARY_TASK else (4,)\n",
    "    input_datatype = (getattr(tf, dtype), getattr(tf, dtype))\n",
    "    dataset = tf.data.Dataset.from_generator(torch_data_gen,\n",
    "                                             output_types=input_datatype,\n",
    "                                             output_shapes=(input_shape, label_shape)\n",
    "                                             )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class OASIS_TF:\n",
    "    def __init__(self):\n",
    "        self.classnames = None\n",
    "        self.num_classes = None\n",
    "        self.num_items = None\n",
    "\n",
    "    def tf_oasis_load_dataset(\n",
    "            self,\n",
    "            directory,\n",
    "            transforms,\n",
    "            label_mode=\"category\",\n",
    "            class_names=None,\n",
    "            color_mode=\"rgb\",\n",
    "            batch_size=32,\n",
    "            image_size=(256, 256),\n",
    "            seed=None,\n",
    "            validation_split=0.2,\n",
    "    ):\n",
    "        assert color_mode in (\"greyscale\", \"rgb\"), color_mode\n",
    "        assert label_mode in (OASISTorchDataset.BINARY_TASK, OASISTorchDataset.CATEGORY_TASK), label_mode\n",
    "        nch = 3 if color_mode == \"rgb\" else 1\n",
    "        dtype = \"float32\"\n",
    "        # load torch dataset\n",
    "        torch_dataset = OASISTorchDataset(directory,\n",
    "                                          task_type=label_mode,\n",
    "                                          image_size=image_size,\n",
    "                                          transforms=transforms,\n",
    "                                          nch=nch,\n",
    "                                          seed=seed,\n",
    "                                          class_names=class_names,\n",
    "                                          dtype=dtype)\n",
    "        self.classnames = torch_dataset.classnames\n",
    "        self.num_classes = torch_dataset.num_classes\n",
    "        self.num_items = len(torch_dataset)\n",
    "        print(\"Number of patients:\", self.num_items)\n",
    "        print(\"Class Names:\", self.classnames)\n",
    "        print(\"Number of classes:\", self.num_classes)\n",
    "        print(f\"Minimum data waste for {batch_size=} is\", self.num_items % batch_size)\n",
    "        # convert torch to tf dataset\n",
    "        dataset = torch_to_tf(torch_dataset,\n",
    "                              task_type=label_mode,\n",
    "                              image_size=image_size,\n",
    "                              nch=nch, dtype=\"float32\")\n",
    "        # calculate split sizes\n",
    "        dataset_size = len(torch_dataset)  # Assuming dataset has a defined length\n",
    "        train_size = int(1. - validation_split * dataset_size)\n",
    "        # split\n",
    "        train_dataset = dataset.take(train_size)\n",
    "        val_dataset = dataset.skip(train_size)\n",
    "        # batch\n",
    "        train_dataset = train_dataset.batch(batch_size)\n",
    "        val_dataset = val_dataset.batch(batch_size)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading optimization and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">82</span> patients, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5002</span> images scanned from <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/soumensardar/Downloads/OASIS/Mild Dementia'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For \u001b[1;36m82\u001b[0m patients, \u001b[1;36m5002\u001b[0m images scanned from \u001b[32m'/Users/soumensardar/Downloads/OASIS/Mild Dementia'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">225</span> patients, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13725</span> images scanned from <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/soumensardar/Downloads/OASIS/Very mild Dementia'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For \u001b[1;36m225\u001b[0m patients, \u001b[1;36m13725\u001b[0m images scanned from \u001b[32m'/Users/soumensardar/Downloads/OASIS/Very mild Dementia'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> patients, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">488</span> images scanned from <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/soumensardar/Downloads/OASIS/Moderate Dementia'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For \u001b[1;36m8\u001b[0m patients, \u001b[1;36m488\u001b[0m images scanned from \u001b[32m'/Users/soumensardar/Downloads/OASIS/Moderate Dementia'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1102</span> patients, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67222</span> images scanned from <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/soumensardar/Downloads/OASIS/Non Demented'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For \u001b[1;36m1102\u001b[0m patients, \u001b[1;36m67222\u001b[0m images scanned from \u001b[32m'/Users/soumensardar/Downloads/OASIS/Non Demented'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Number of patients: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1417</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Number of patients: \u001b[1;36m1417\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Class Names:\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Mild Dementia'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Very mild Dementia'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Moderate Dementia'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Non Demented'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Class Names:\n",
       "\u001b[1m[\u001b[0m\u001b[32m'Mild Dementia'\u001b[0m, \u001b[32m'Very mild Dementia'\u001b[0m, \u001b[32m'Moderate Dementia'\u001b[0m, \u001b[32m'Non Demented'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Number of classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Number of classes: \u001b[1;36m4\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Minimum data waste for <span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span> is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Minimum data waste for \u001b[33mbatch_size\u001b[0m=\u001b[1;36m64\u001b[0m is \u001b[1;36m9\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bilateral_filter(pil_image):\n",
    "    return cv2.bilateralFilter(np.array(pil_image), 15, 75, 75)\n",
    "\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.Lambda(bilateral_filter),\n",
    "        torchvision.transforms.Normalize(mean=0., std=1.),\n",
    "])\n",
    "\n",
    "odd = OASIS_TF()\n",
    "\n",
    "train_val_ds, test_ds = odd.tf_oasis_load_dataset(\n",
    "    datasetdir,\n",
    "    transforms=transforms,\n",
    "    label_mode=task_type,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(image_size, image_size),\n",
    "    seed=random_seed,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "CLASS_NAMES = odd.classnames\n",
    "num_classes = odd.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Mild Dementia'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Very mild Dementia'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Moderate Dementia'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Non Demented'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'Mild Dementia'\u001b[0m, \u001b[32m'Very mild Dementia'\u001b[0m, \u001b[32m'Moderate Dementia'\u001b[0m, \u001b[32m'Non Demented'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(CLASS_NAMES)\n",
    "\n",
    "# Reducing dataset size for ablation study\n",
    "if ablation_study_size > 0:\n",
    "    batch_count = int(round(ablation_study_size / batch_size, 0))\n",
    "    train_val_ds = train_val_ds.take(batch_count)\n",
    "\n",
    "# Create a test dataset by further splitting the validation set\n",
    "val_batches = tf.data.experimental.cardinality(train_val_ds)\n",
    "\n",
    "val_ds = train_val_ds.take(val_batches // 5) # 20% of validation as test\n",
    "train_ds = train_val_ds.skip(val_batches // 5)\n",
    "\n",
    "\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('class_weights', None, '==============')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = None\n",
    "\"class_weights\", class_weights, \"==============\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Logger\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "from tensorflow.keras.callbacks import LambdaCallback, Callback, LearningRateScheduler\n",
    "import shutil, glob\n",
    "\n",
    "# Create EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", \n",
    "                               mode=\"min\", \n",
    "                               patience=7, \n",
    "                               min_delta=0.001,\n",
    "                               verbose=1,\n",
    "                               start_from_epoch=9,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "csv_logger = CSVLogger(artifact_root + '/metrics.csv')\n",
    "# Tensorboard Logger\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=tf_log_dir,\n",
    "    histogram_freq=0,\n",
    "    write_graph=False,\n",
    "    write_images=False,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None    \n",
    ")\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    artifact_root + \"/models/epoch={epoch:02d}.h5\",\n",
    "    monitor=monitor,\n",
    "    verbose=0,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode=mode,\n",
    "    save_freq=freq,\n",
    "    initial_value_threshold=initial_threshold,\n",
    ")\n",
    "\n",
    "model_best_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    artifact_root + \"/models/best-model.h5\",\n",
    "    monitor=monitor,\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=mode,\n",
    "    save_freq=freq,\n",
    "    initial_value_threshold=initial_threshold,\n",
    ")\n",
    "\n",
    "class BestModelEpochTrackerCallback(Callback):\n",
    "    \"\"\"\n",
    "    This callback monitor best values and updates in a json file project_dir/bestvalue.json\n",
    "    \"\"\"\n",
    "    def __init__(self, monitor, mode, initial_value_threshold=None, verbose=0):\n",
    "        assert mode in (\"min\", \"max\")\n",
    "        initial_thresh = initial_value_threshold\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best_value = initial_thresh\n",
    "        if mode == \"min\":\n",
    "            self.is_better = np.less\n",
    "            if self.best_value is None:\n",
    "                self.best_value = np.Inf\n",
    "        elif mode == \"max\":\n",
    "            self.is_better = np.greater\n",
    "            if self.best_value is None:\n",
    "                self.best_value = -np.Inf \n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def on_epoch_end(self, epoch, metrics=None):\n",
    "        curr_val = metrics.get(self.monitor, None)\n",
    "        assert curr_val is not None, f\"Unable to find the metric to monitor: {self.monitor}\"\n",
    "        if self.is_better(curr_val, self.best_value):\n",
    "            update_path = artifact_root + \"/bestvalue.json\"\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}: {self.monitor} improved form {self.best_value:.5f} to {curr_val:.5f} and saving updates to {update_path}\")\n",
    "            self.best_value = curr_val\n",
    "            with open(update_path, \"w\") as fp:\n",
    "                json.dump(dict(epoch=epoch+1, \n",
    "                               monitor=self.monitor, \n",
    "                               value=curr_val, \n",
    "                               mode=self.mode, \n",
    "                               frequency=\"epoch\"\n",
    "                              ), fp, indent=4)\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                self.print(f\"Epoch {epoch+1}: {self.monitor} did not improved form {self.best_value}\")\n",
    "            \n",
    "                \n",
    "\n",
    "bestval_monitor_callback = BestModelEpochTrackerCallback(\n",
    "    monitor=monitor,\n",
    "    mode=mode,\n",
    "    initial_value_threshold=initial_threshold,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class CleanupCallback(Callback):\n",
    "    def on_epoch_begin(self, epoch, metrics=None):\n",
    "        # clean last checkpoint assets\n",
    "        last_epoch = epoch - 1\n",
    "        # update current\n",
    "        with open(artifact_root + \"/currentepoch.txt\", \"w\") as fp:\n",
    "            fp.write(f\"{epoch}\")\n",
    "        # look for last epoch checkpoint and delete\n",
    "        pattern = artifact_root + \"/models/epoch=%02d.h5\" % (last_epoch)\n",
    "        if os.path.exists(pattern):\n",
    "            os.remove(pattern)\n",
    "        \n",
    "        \n",
    "cleanup_callback = CleanupCallback()\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "if lr_schedular:\n",
    "    callbacks = [lr_callback]\n",
    "else:\n",
    "    callbacks = []\n",
    "\n",
    "callbacks.extend([csv_logger, tensorboard_callback, model_ckpt, model_best_ckpt, bestval_monitor_callback, cleanup_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3D build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "422-4FV2I3fc",
    "outputId": "302edc0a-3d72-411d-96d3-f491243b826f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Building model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Building model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Building model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Building model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.applications' has no attribute '3DMobileNet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mK\u001b[39;00m\n\u001b[1;32m      6\u001b[0m input_t\u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(image_size,image_size,n_ch))\n\u001b[0;32m----> 7\u001b[0m res_model\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplications\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m(include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_tensor\u001b[38;5;241m=\u001b[39minput_t)\n\u001b[1;32m      8\u001b[0m model_lenet \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[1;32m      9\u001b[0m model_lenet\u001b[38;5;241m.\u001b[39madd(res_model)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.applications' has no attribute '3DMobileNet'"
     ]
    }
   ],
   "source": [
    "# Define the model architecture (ResNet50)\n",
    "# import tensorflow.keras as K\n",
    "log(\"Building model...\")\n",
    "import keras as K\n",
    "    \n",
    "input_t= K.Input(shape=(image_size,image_size,n_ch))\n",
    "res_model= getattr(K.applications, model_name)(include_top=False, weights=\"imagenet\", input_tensor=input_t)\n",
    "model_lenet = K.models.Sequential()\n",
    "model_lenet.add(res_model)\n",
    "model_lenet.add(K.layers.Flatten())\n",
    "model_lenet.add(K.layers.Dense(num_classes, activation=activation))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "log(\"Compile model\")\n",
    "model_lenet.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "ngUGLapkJJTS",
    "outputId": "ac1af57f-8bc0-4507-d1ab-60a63d33881b"
   },
   "outputs": [],
   "source": [
    "# reload checkpoint\n",
    "if resume_checkpoint_path and os.path.exists(resume_checkpoint_path):\n",
    "    log(\"Resuming training...\", resume_checkpoint_path)\n",
    "    model_lenet.load_weights(resume_checkpoint_path)\n",
    "else:\n",
    "    log(\"Fresh training...\")\n",
    "    initial_epoch = 0\n",
    "# daving hyper-params\n",
    "save_hparams()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "log(\"Experiment: Started\", time=True)\n",
    "log(f\"Starting training model={model_name}\")\n",
    "history = model_lenet.fit(train_ds, epochs=max_epoch, \n",
    "                          initial_epoch=initial_epoch, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks = callbacks,\n",
    "                          class_weight=class_weights,\n",
    "                          validation_data=val_ds)\n",
    "log(f\"Training done={model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "5wxWcmISJaLB",
    "outputId": "4c5daded-5a68-4990-c8d2-bc1b4ba85d89"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "log(\"Experiment: Evaluating\", time=True)\n",
    "log(f\"Evaluating model={model_name}...\")\n",
    "test_result = model_lenet.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='loss', **kwargs):\n",
    "        super().__init__(name='loss', **kwargs)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        pass\n",
    "\n",
    "    def result(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "sYKD-JQrJaOn",
    "outputId": "780d0265-f3b2-43bf-9cf6-de99785400ed"
   },
   "outputs": [],
   "source": [
    "metrics.append(LossMetric())\n",
    "\n",
    "log(f\"{model_name}\", {f\"test_{k.name}\":v for v,k in zip(test_result, metrics)})\n",
    "\n",
    "# TF-logging test values\n",
    "file_writer = tf.summary.create_file_writer(tf_log_dir + \"/test\")\n",
    "with file_writer.as_default():\n",
    "    for v, k in zip(test_result, metrics):\n",
    "        tf.summary.scalar(f\"test_{k.name}\", v, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "Y_AmXWIKJpgw",
    "outputId": "406d1461-664c-4991-a208-dbebad70e3b3"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def preprocess_image(image_path, target_size=(image_size, image_size)):\n",
    "    # Open the image file\n",
    "    img = Image.open(image_path)\n",
    "    # Resize the image\n",
    "    img = img.resize(target_size)\n",
    "    # Convert the image to a NumPy array\n",
    "    img_array = np.array(img)\n",
    "    # Expand dimensions to match the shape expected by the model (if necessary)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "actual_class = TEST_IMG_LABEL\n",
    "image_path = TEST_IMG_PATH\n",
    "image_array = preprocess_image(image_path)\n",
    "log(\"Shape of preprocessed image array:\", image_array.shape)\n",
    "\n",
    "# Predict probabilities\n",
    "log(\"Experiment: Testing\", time=True)\n",
    "prediction_probabilities = model_lenet.predict(image_array)\n",
    "\n",
    "# Get the index of the highest probability\n",
    "predicted_class_index = np.argmax(prediction_probabilities)\n",
    "\n",
    "# Define your class labels\n",
    "class_labels = CLASS_NAMES\n",
    "\n",
    "# Map the index to the corresponding class label\n",
    "predicted_class = class_labels[predicted_class_index]\n",
    "\n",
    "# TF-logging test values\n",
    "file_writer = tf.summary.create_file_writer(tf_log_dir + \"/test\")\n",
    "with file_writer.as_default():\n",
    "    tf.summary.text(f\"Corner Case\", f\"Actual={actual_class} Predicted={predicted_class}\", step=0)\n",
    "\n",
    "log(\"Actual class:\", actual_class)\n",
    "log(\"Predicted class:\", predicted_class)\n",
    "log(\"Experiment: Completed\", time=True)\n",
    "\n",
    "with file_writer.as_default():\n",
    "    # Don't forget to reshape.\n",
    "    figure = plt.figure(figsize=(5,5))\n",
    "    images = np.reshape(image_array, (image_size, image_size, n_ch))\n",
    "    plt.imshow(images)\n",
    "    plt.title(f\"Actual={actual_class} vs Predicted={predicted_class}\")\n",
    "    # TODO add actual vs predicted label\n",
    "    images = plot_to_image(figure)\n",
    "    tf.summary.image(f\"Test Image\", images, max_outputs=1, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(f\"Your result is saved at: '{artifact_root}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start tensorboard\n",
    "# # Load the TensorBoard notebook extension\n",
    "# %reload_ext tensorboard\n",
    "# if SELECTED_DATASET == SMALL_DATASET:\n",
    "#     %tensorboard --logdir \"results/tflogs_smalldataset/\" \n",
    "# elif SELECTED_DATASET == FULL_DATASET:\n",
    "#     %tensorboard --logdir \"results/tflog/\"\n",
    "# elif SELECTED_DATASET == FLOWER_DATASET:\n",
    "#     %tensorboard --logdir \"results/tflogs_flower_dataset/\" --port 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\"ResNet50\", \"VGG16\", \"InceptionResnetV2\", \"Mobilenet\"]\n",
    "# batch_size = [32, 64, 1024]\n",
    "# loss_func = [ \"categorical_cross_entropy\", ]\n",
    "# optimizer = [\"adam\", ]\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# for mod in models:\n",
    "#     for loss_ in loss_func:\n",
    "#         for op in optimizer:\n",
    "#             for bs in batch_size:\n",
    "#                 print(dict(model=mod, loss_func = loss_, optimizer=op, batch_sieze=bs, lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
