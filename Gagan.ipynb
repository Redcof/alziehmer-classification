{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soumensardar/miniconda3/envs/tf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# print(\"Current branch name: \")\n",
    "# !git log -1 --oneline\n",
    "\n",
    "try:\n",
    "    from rich import print\n",
    "except:\n",
    "    !pip install rich\n",
    "    from rich import print\n",
    "\n",
    "try:\n",
    "    import tensorflow_datasets, tensorflow, matplotlib, numpy, PIL, sklearn\n",
    "except:\n",
    "    !pip install tensorflow_datasets tensorflow matplotlib numpy pillow scikit-learn\n",
    "    import tensorflow_datasets, tensorflow, matplotlib, numpy, PIL, sklearn\n",
    "\n",
    "try:\n",
    "    import keras_tuner as kt\n",
    "except:\n",
    "    !pip install keras-tuner\n",
    "    import keras_tuner as kt\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    !pip install opencv-python\n",
    "    import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision.transforms\n",
    "except:\n",
    "    !pip install torch torchvision\n",
    "    import torch\n",
    "    import torchvision.transforms\n",
    "\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Selecting dataset automatically based on platform = <span style=\"color: #008000; text-decoration-color: #008000\">'Darwin'</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Selecting dataset automatically based on platform = \u001b[32m'Darwin'\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DONE\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DONE\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT TOUCH BELOW ==================================\n",
    "AUTO_SELECT = -1\n",
    "FULL_DATASET = 0\n",
    "FULL_BINARY_DATASET = 1\n",
    "MY_DATASET = 2\n",
    "MY_BIN_DATASET = 3\n",
    "# DO NOT TOUCH - END ===================================\n",
    "\n",
    "# select dataset\n",
    "SELECTED_DATASET = AUTO_SELECT # CHANGE HERE\n",
    "\n",
    "if SELECTED_DATASET == AUTO_SELECT:\n",
    "    print(f\"Selecting dataset automatically based on platform = '{platform.system()}'...\")\n",
    "    if platform.system() == \"Windows\":\n",
    "        SELECTED_DATASET = FULL_DATASET\n",
    "    elif platform.system() == \"Darwin\":\n",
    "        SELECTED_DATASET = MY_DATASET\n",
    "    else:\n",
    "        raise Exception(\"{SELECTED_DATASET=AUTO_SELECT} is not supported for this platform.\")\n",
    "    print(\"DONE\")\n",
    "\n",
    "\n",
    "# DO NOT TOUCH BELOW ==================================\n",
    "FULL_DATASET_PATH = r'D:\\CGC work\\Alziehmer Disease\\PhdNotebook\\OASIS dataset\\OASIS dataset\\Data'\n",
    "FULL_BINARY_DATASET_PATH = r'D:\\CGC work\\Alziehmer Disease\\PhdNotebook\\OASIS dataset Binary\\OASIS dataset\\Data'\n",
    "MY_DATASET_PATH = r'/Users/soumensardar/Downloads/OASIS/'\n",
    "MY_DATASET_BINARY_PATH = r'/Users/soumensardar/Downloads/OASIS-binary/'\n",
    "\n",
    "# while adding more datasets, make sure to add tflog directory\n",
    "datasetdir, dataset_name =[  (FULL_DATASET_PATH, 'tflogs_mvcnn'),\n",
    "                             (FULL_BINARY_DATASET_PATH, 'tflogs_binary_mvcnn'),\n",
    "                             (MY_DATASET_PATH, 'tflogs_mvcnn'),\n",
    "                             (MY_DATASET_BINARY_PATH, 'tflogs_binary_mvcnn'),\n",
    "                    ][SELECTED_DATASET]\n",
    "# DO NOT TOUCH - END ===================================\n",
    "datasetdir, dataset_name\n",
    "assert os.path.exists(datasetdir), f\"Dataset path is incorrect {datasetdir}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Image selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/soumensardar/Downloads/OASIS/Mild Dementia/OAS1_0028_MR1_mpr-1_127.jpg',\n",
       " 'Mild Dementia')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_IMG_PATH, TEST_IMG_LABEL = [\n",
    "(r'D:\\CGC work\\Alziehmer Disease\\PhdNotebook\\OASIS dataset\\OASIS dataset\\Data\\Mild Dementia/OAS1_0028_MR1_mpr-1_127.jpg', 'Mild_Dementia'),\n",
    "(r'D:\\CGC work\\Alziehmer Disease\\PhdNotebook\\OASIS dataset Binary\\OASIS dataset\\Data\\1. Mild Dementia/OAS1_0028_MR1_mpr-1_127.jpg', '1. Mild_Dementia'),\n",
    "(r'/Users/soumensardar/Downloads/OASIS/Mild Dementia/OAS1_0028_MR1_mpr-1_127.jpg', 'Mild Dementia'),\n",
    "(r'/Users/soumensardar/Downloads/OASIS-binary/Mild Dementia/OAS1_0028_MR1_mpr-1_127.jpg', '1. Mild Dementia'),\n",
    "][SELECTED_DATASET]\n",
    "import os\n",
    "assert os.path.exists(TEST_IMG_PATH), f\"Test image path is incorrect {TEST_IMG_PATH}\"\n",
    "TEST_IMG_PATH, TEST_IMG_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "def get_experiment_details(dataset, model, ts):\n",
    "    exp_asset_dir = f\"results/{dataset}/{model}/{ts}\"\n",
    "    assert os.path.exists(exp_asset_dir), f\"Experiment does not exist '{exp_asset_dir}'\"\n",
    "    checkpoint_model_dir = None\n",
    "    with open(exp_asset_dir + \"/currentepoch.txt\") as fp:\n",
    "        last_epoch = int(fp.read().strip())\n",
    "        checkpoint_model_dir = exp_asset_dir + \"/models/epoch=%02d.h5\"%last_epoch\n",
    "        assert os.path.exists(checkpoint_model_dir), f\"Unable to find the last checkpoint file {checkpoint_model_dir}\"\n",
    "    best_model_dir = exp_asset_dir + \"/models/best-model.h5\"\n",
    "    if not os.path.exists(best_model_dir):\n",
    "        best_model_dir = None\n",
    "\n",
    "    best_model_info = None\n",
    "    if os.path.exists(exp_asset_dir + \"/bestvalue.json\"):\n",
    "        with open(exp_asset_dir + \"/bestvalue.json\") as fp:\n",
    "            best_model_info = json.load(fp)\n",
    "        \n",
    "    return dict(last_epoch=last_epoch, \n",
    "                best_checkpoint=best_model_dir, \n",
    "                last_checkpoint=checkpoint_model_dir,\n",
    "                best_model_info=best_model_info,\n",
    "                project_dir=exp_asset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">what_changed</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Training with model_name='MultiViewMobileNet' task_type='categorical' lr_schedular=True </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">activation='softmax' loss_function='categorical_crossentropy'\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mwhat_changed\u001b[0m=\u001b[32m\"Training\u001b[0m\u001b[32m with \u001b[0m\u001b[32mmodel_name\u001b[0m\u001b[32m='MultiViewMobileNet' \u001b[0m\u001b[32mtask_type\u001b[0m\u001b[32m='categorical' \u001b[0m\u001b[32mlr_schedular\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mactivation\u001b[0m\u001b[32m='softmax' \u001b[0m\u001b[32mloss_function\u001b[0m\u001b[32m='categorical_crossentropy'\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_seed=37\n",
    "resume_training_timestamp = None # CHANGE HERE\n",
    "max_epoch = 50 # CHANGE HERE\n",
    "batch_size = 13\n",
    "image_size = 128\n",
    "color_mode = 'rgb'\n",
    "n_ch = dict(rgb=3, grayscale=1)[color_mode]\n",
    "volume_depth = 61\n",
    "monitor = \"accuracy\"\n",
    "initial_threshold = 0.5\n",
    "mode=\"max\"\n",
    "freq=\"epoch\"\n",
    "initial_epoch=0\n",
    "precision = 32 # 16 bits or 32 bits\n",
    "\n",
    "learning_rate = 0.001\n",
    "lr_schedular = False\n",
    "early_stop = True\n",
    "enable_class_weight = False\n",
    "ablation_study_size = 0 # batch_size * 10 # CHANGE HERE\n",
    "\n",
    "task_type = \"categorical\" # \"binary\"\n",
    "# Link: https://www.tensorflow.org/api_docs/python/tf/keras/losses#functions\n",
    "if task_type == \"binary\":\n",
    "    activation = \"sigmoid\"\n",
    "    loss_function = \"binary_crossentropy\"\n",
    "else:\n",
    "    activation = \"softmax\"\n",
    "    loss_function = \"categorical_crossentropy\"\n",
    "\n",
    "resume_checkpoint_path = None\n",
    "# Link: https://keras.io/api/applications/\n",
    "model_name=\"MultiViewMobileNet\" # CHANGE HERE !!!!!!!\n",
    "# add a comment about what changes you have done just now before running the training\n",
    "what_changed = f\"Training with {model_name=} {task_type=} {lr_schedular=} {activation=} {loss_function=}\"\n",
    "print(f\"{what_changed=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "2024-12-23 15:31:09.190863: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-12-23 15:31:09.190882: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-23 15:31:09.190885: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-23 15:31:09.190912: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-23 15:31:09.190924: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# EACH LINK CONTAINS AVAILABLE OPTIONS\n",
    "# Link: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers#classes\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate) # CHANGE HERE\n",
    "# Link: https://www.tensorflow.org/api_docs/python/tf/keras/metrics#classes\n",
    "metrics = [tf.keras.metrics.Accuracy(), \n",
    "           tf.keras.metrics.Recall(), \n",
    "           tf.keras.metrics.Precision(),\n",
    "           tf.keras.metrics.F1Score(average=\"macro\"),\n",
    "           tf.keras.metrics.SensitivityAtSpecificity(0.6),\n",
    "           tf.keras.metrics.SpecificityAtSensitivity(0.6),\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedular callback\n",
    "def lr_schedule(epoch):\n",
    "  \"\"\"\n",
    "  Returns a custom learning rate that decreases as epochs progress.\n",
    "  \"\"\"\n",
    "  learning_rate = 0.02\n",
    "  if epoch > 5:\n",
    "    learning_rate = 0.01\n",
    "  if epoch > 10:\n",
    "    learning_rate = 0.0001\n",
    "  if epoch > 15:\n",
    "    learning_rate = 0.00001\n",
    "\n",
    "  tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "  return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_training_timestamp:\n",
    "    print(f\"Trying to resume from checkpoint... {resume_training_timestamp}\")\n",
    "    d = get_experiment_details(dataset_name, model_name, resume_training_timestamp)\n",
    "    initial_epoch = d['last_epoch']\n",
    "    resume_checkpoint_path = d['last_checkpoint']\n",
    "    assert os.path.exists(resume_checkpoint_path), f\"Unable to resume training from '{d['project_dir']}'.\"\n",
    "    best_model_info = d['best_model_info']\n",
    "    if best_model_info:\n",
    "        print(\"Updating the metric monitoring parameters before resuming the checkpoint\")\n",
    "        monitor = best_model_info['monitor']\n",
    "        initial_threshold = best_model_info['value']\n",
    "        mode=best_model_info['mode']\n",
    "        freq=best_model_info['frequency']\n",
    "    print(f\"Resuming checkpoint form epoch={initial_epoch}.\")\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "def save_hparams():\n",
    "    hyprams = dict(\n",
    "            timestamp = timestamp,\n",
    "            what_changed=what_changed,\n",
    "            random_state=random_seed,\n",
    "            max_epoch=max_epoch,\n",
    "            batch_size=batch_size,\n",
    "            image_size=image_size,\n",
    "            num_channels=n_ch,\n",
    "            metrics=str(metrics),\n",
    "            loss_function=loss_function,\n",
    "            monitoring=dict(monitor = monitor, initial_threshold = initial_threshold, mode=mode),\n",
    "            resume_training_timestamp=resume_training_timestamp,\n",
    "            initial_epoch=initial_epoch,\n",
    "            model_name=model_name,\n",
    "            optimizer=optimizer.name,\n",
    "            learning_rate=learning_rate,\n",
    "            lr_schedular=lr_schedular,\n",
    "        )\n",
    "    import json \n",
    "    log(\"Saving hyperparameters.\")\n",
    "    print(hyprams)\n",
    "    # Convert and write JSON object to file\n",
    "    with open(f\"{artifact_root}/hyperparams.json\", \"w\") as outfile: \n",
    "        json.dump(hyprams, outfile, indent=4)\n",
    "    file_writer = tf.summary.create_file_writer(tf_log_dir + \"/hparams\")\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.text(\"hyperparams.json\", f\"{artifact_root}/hyperparams.json\", step=0)\n",
    "        for k, v in hyprams.items():\n",
    "            if isinstance(v, int):\n",
    "                tf.summary.scalar(k, v, step=0)\n",
    "            elif isinstance(v, float):\n",
    "                tf.summary.scalar(k, v, step=0)\n",
    "            else:\n",
    "                tf.summary.text(k, str(v), step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare artifact directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Experiment path: <span style=\"color: #008000; text-decoration-color: #008000\">'results/tflogs_mvcnn/MultiViewMobileNet/20241223-153109'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Experiment path: \u001b[32m'results/tflogs_mvcnn/MultiViewMobileNet/20241223-153109'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Experiment path: <span style=\"color: #008000; text-decoration-color: #008000\">'results/tflogs_mvcnn/MultiViewMobileNet/20241223-153109'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Experiment path: \u001b[32m'results/tflogs_mvcnn/MultiViewMobileNet/20241223-153109'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if resume_training_timestamp:\n",
    "    print(\"Resume timestamp\", resume_training_timestamp)\n",
    "    timestamp = resume_training_timestamp\n",
    "unique_dir = f\"{model_name}/{timestamp}\"\n",
    "tf_log_dir = f\"results/{dataset_name}/{unique_dir}\"\n",
    "tf_log_img_dir = f\"results/{dataset_name}/images\"\n",
    "artifact_root  = f\"results/{dataset_name}/{unique_dir}\"\n",
    "pathlib.Path(artifact_root).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(tf_log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def log(*args, **kwargs):\n",
    "    time = False\n",
    "    if \"time\" in kwargs.keys():\n",
    "        time = kwargs[\"time\"]\n",
    "        del kwargs[\"time\"]\n",
    "    if time:\n",
    "        time = datetime.datetime.now().strftime(\"%H:%M:%S %d/%m/%Y\")\n",
    "        a = list(args)\n",
    "        a.append(time)\n",
    "        args = tuple(a)\n",
    "    print(*args, **kwargs)\n",
    "    with open(f\"{artifact_root}/additional_logs.txt\", \"a\") as fp:\n",
    "        kwargs[\"file\"] = fp\n",
    "        kwargs[\"flush\"] = True\n",
    "        print(*args, **kwargs)\n",
    "log(f\"Experiment path: '{artifact_root}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Training Resume Timestamp\n",
    "## Use this `timestamp` to update `resume_training_timestamp` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Resume timestamp: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20241223</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">153109</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Resume timestamp: \u001b[1;36m20241223\u001b[0m-\u001b[1;36m153109\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Resume timestamp: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20241223</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">153109</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Resume timestamp: \u001b[1;36m20241223\u001b[0m-\u001b[1;36m153109\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log(f\"Resume timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D6HlWjJDFnvy"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
    "#   DataGenerator to read images and rescale images\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import random\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading 3D datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torchvision.transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class OASISTorchDataset(Dataset):\n",
    "    BINARY_TASK = \"binary\"\n",
    "    CATEGORY_TASK = \"categorical\"\n",
    "    VOLUME_DEPTH = 61  # KNOWN DURING EDA: per patient we have 61 MRI slices\n",
    "\n",
    "    def get_volume_for_image(self, oasis_image_path):\n",
    "        \"\"\"\n",
    "        For a given image this function automatically determine the slices and return the volume and actual label\n",
    "        :param oasis_image_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        oasis_image_path = pathlib.Path(oasis_image_path)\n",
    "        class_name = oasis_image_path.parent.name\n",
    "        # OAS1_0001_MR1_mpr-1_101.jpg\n",
    "        fp, sp = oasis_image_path.name.split(\"-\")  # [OAS1_0001_MR1_mpr, 1_101.jpg]\n",
    "        spfp, spsp = sp.split(\"_\")  # [1, 101.jpg]\n",
    "        pattern = fp + \"-\" + spfp + \"*\"\n",
    "        images = list(oasis_image_path.parent.glob(pattern))\n",
    "        images = self._sample_from(images, OASISTorchDataset.VOLUME_DEPTH)\n",
    "        lbl = self.encode_labels(class_name)\n",
    "        return self.load_volume(images), class_name, lbl\n",
    "\n",
    "    def __init__(self, path, task_type, image_size, transforms, split_name, batch_size, ablation=0, nch=3, seed=37,\n",
    "                 splits=(70, 20, 10),\n",
    "                 class_names=None,\n",
    "                 dtype=\"float32\", verbose=0):\n",
    "        assert len(image_size) == 2, \"Image size must be a tuple of two integers\"\n",
    "        assert split_name in (\"train\", \"validation\", \"test\")\n",
    "        assert nch in (1, 3), \"Supported channels are 1(greyscale) and 3(rgb)\"\n",
    "        assert len(splits) == 3 and sum(\n",
    "            splits) == 100, f\"train-validation-test splits must accumulate to 100, but {splits=} given.\"\n",
    "        self.path = pathlib.Path(path)\n",
    "        self.verbose = verbose\n",
    "        # get class names\n",
    "        self.classnames = self._load_class_names()\n",
    "        if class_names is not None:\n",
    "            self.classnames = [c for c in self.classnames if c in class_names]\n",
    "        if task_type == self.BINARY_TASK and len(self.classnames) >= 2 and class_names is None:\n",
    "            raise Exception(f\"When `class_names` is not specified, \"\n",
    "                            f\"only 2 classes are allowed in the data-directory, but {len(self.classnames)} found.\")\n",
    "        self.num_classes = len(self.classnames)\n",
    "        if task_type == self.BINARY_TASK:\n",
    "            self.num_classes = 1\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.dtype = dtype\n",
    "        self.batch_size = batch_size\n",
    "        self.transforms = transforms if transforms else torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean=0., std=1.),\n",
    "        ])\n",
    "        self.split_name = split_name\n",
    "        self.task_type = task_type\n",
    "        self.nch = nch\n",
    "        self.splits = splits\n",
    "        self.items = []\n",
    "        self.ablation = ablation\n",
    "        # load dataset\n",
    "        self._load(seed)\n",
    "\n",
    "    def _load_class_names(self):\n",
    "        return [i for i in os.listdir(self.path) if i != \".DS_Store\"]\n",
    "\n",
    "    def _group_images_by_subject(self, image_dir):\n",
    "        \"\"\"Groups images by subject ID based on their filenames.\n",
    "        Args:\n",
    "            image_dir: Path to the directory containing the images.\n",
    "        Returns:\n",
    "            A dictionary where keys are subject IDs and values are lists of image filenames.\n",
    "        \"\"\"\n",
    "        image_files = os.listdir(image_dir)\n",
    "        subject_groups = {}\n",
    "        for filename in image_files:\n",
    "            # Extract subject ID from the filename (adjust the pattern as needed)\n",
    "            # OAS1_0001_MR1_mpr-1_101.jpg\n",
    "            subject_id = \"_\".join(filename.split('_')[0:-1])\n",
    "            if subject_id not in subject_groups:\n",
    "                subject_groups[subject_id] = []\n",
    "            subject_groups[subject_id].append(os.path.join(image_dir, filename))\n",
    "        if self.verbose:\n",
    "            print(\"For\", len(subject_groups.keys()), \"patients,\", len(image_files),\n",
    "                  f\"images scanned from '{image_dir}'\")\n",
    "        return subject_groups\n",
    "\n",
    "    def item_generator(self):\n",
    "        \"\"\"\n",
    "        :return: yield MRI slices as list and encoded label\n",
    "        \"\"\"\n",
    "        for class_dir in self.classnames:\n",
    "            grouped_files = self._group_images_by_subject((self.path / class_dir).resolve())\n",
    "            for slices, label in zip(grouped_files.values(), [class_dir] * len(grouped_files.values())):\n",
    "                lbl = self.encode_labels(label)\n",
    "                yield self._sample_from(slices, OASISTorchDataset.VOLUME_DEPTH), lbl\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample_from(lst, n):\n",
    "        if n > len(lst):\n",
    "            raise ValueError(\"Sample size cannot be greater than the list size.\")\n",
    "\n",
    "            # Calculate step size for approximately even distribution\n",
    "        step_size = len(lst) // n\n",
    "\n",
    "        # Generate indices with approximately even spacing\n",
    "        indices = [i * step_size for i in range(n)]\n",
    "\n",
    "        # Adjust last index to ensure it's within bounds\n",
    "        indices[-1] = min(indices[-1], len(lst) - 1)\n",
    "        return [lst[i] for i in indices]\n",
    "\n",
    "    def encode_labels(self, labels):\n",
    "        \"\"\"\n",
    "        :param labels: this could be a string or list of strings\n",
    "        :return: encoded binary tensor of [1,0,1,...] or [[0,1],[1,0],[0,1],...]\n",
    "                for categorical\n",
    "        \"\"\"\n",
    "        is_one_element = False\n",
    "        if isinstance(labels, str):\n",
    "            labels = [labels]\n",
    "            is_one_element = True\n",
    "        enc_label = []\n",
    "        for lbl in labels:\n",
    "            if lbl not in self.classnames:\n",
    "                warnings.warn(f\"'{lbl}' label is unknown\")\n",
    "            if self.task_type == self.BINARY_TASK:\n",
    "                enc_label.append(self.classnames.index(lbl))\n",
    "            else:\n",
    "                enc_label.append([lbl == cls_nm for cls_nm in self.classnames])\n",
    "        if is_one_element:\n",
    "            return np.array(enc_label[0]).astype(self.dtype)\n",
    "        else:\n",
    "            return np.array(enc_label).astype(self.dtype)\n",
    "\n",
    "    def decode_labels(self, probabilities: np.ndarray, probability_threshold=0.5):\n",
    "        \"\"\"\n",
    "        :param probabilities: batch of the network output(probabilities)\n",
    "        :param probability_threshold: to decide the class\n",
    "        :return: decode [probas1, probas2, ...] to tensor of [cls1, cls2,...]\n",
    "        \"\"\"\n",
    "        assert len(probabilities.shape) == 2, f\"`probabilities` shape must be two dimensional\"\n",
    "        if self.task_type == self.BINARY_TASK:\n",
    "            assert probabilities.shape[1] == 1, f\"`probabilities` shape must be (batch_size, 1)\"\n",
    "            binaries = probabilities >= probability_threshold\n",
    "            return list(map(lambda oi: self.classnames[int(oi)], binaries))\n",
    "        else:\n",
    "            assert probabilities.shape[1] == len(self.classnames), (f\"`probabilities` shape must be\"\n",
    "                                                                    f\" (batch_size, num_classes)\")\n",
    "            probability_threshold = [probability_threshold] * len(self.classnames)\n",
    "            indices = (probabilities >= probability_threshold).argmax(axis=1)\n",
    "            string_array = np.array(self.classnames)\n",
    "            return string_array[indices].tolist()\n",
    "\n",
    "    def load_volume(self, image_files: list, label=None):\n",
    "        \"\"\"Loads a 3D volume from a directory of JPEG images.\n",
    "        Args:\n",
    "            image_files: list of files\n",
    "        Returns:\n",
    "            A 3D representing the 3D volume\n",
    "        \"\"\"\n",
    "        image_volume = []\n",
    "        for image_path in image_files:\n",
    "            image_data = cv2.imread(image_path)\n",
    "            if self.nch == 1:\n",
    "                image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2GRAY)\n",
    "            else:\n",
    "                image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)\n",
    "            # resize\n",
    "            image_data = cv2.resize(image_data, self.image_size)\n",
    "            # transforming images\n",
    "            if self.transforms is not None:\n",
    "                image_data = self.transforms(image_data.astype(\"uint8\"))\n",
    "            # creating volume\n",
    "            image_volume.append(image_data)\n",
    "        volume = np.stack(image_volume, axis=0).astype(self.dtype).reshape((OASISTorchDataset.VOLUME_DEPTH,\n",
    "                                                                            *self.image_size, self.nch))\n",
    "        if label is None:\n",
    "            return volume\n",
    "        else:\n",
    "            return volume, label\n",
    "\n",
    "    def _collect_items(self):\n",
    "        return list(self.item_generator())\n",
    "\n",
    "    def _get_split_sizes(self, total_items):\n",
    "        size = total_items\n",
    "        # convert [0,100] scale to [0,1]\n",
    "        train_ratio = self.splits[0] / 100\n",
    "        val_ratio = self.splits[1] / 100\n",
    "        test_ratio = self.splits[2] / 100\n",
    "        # calculate no. of batches possible\n",
    "        no_batches = (size // self.batch_size)\n",
    "        # calculate no. of batches per split\n",
    "        train_batches = int(round(no_batches * train_ratio))\n",
    "        val_batches = int(round(no_batches * val_ratio))\n",
    "        test_batches = int(round(no_batches * test_ratio))\n",
    "        # calculate residual batches\n",
    "        residual_batches = train_batches + val_batches + test_batches - no_batches\n",
    "        # adjust residual batches to/from train_batches\n",
    "        if residual_batches < 0:\n",
    "            # if not all batches are used, add remaining batches to train set\n",
    "            test_batches += -residual_batches\n",
    "        if residual_batches > 0:\n",
    "            # if more batches are used, remove extra batches from train set\n",
    "            test_batches -= residual_batches\n",
    "        # now calculate item count per split form batch count\n",
    "        train_size = train_batches * self.batch_size\n",
    "        val_size = val_batches * self.batch_size\n",
    "        test_size = test_batches * self.batch_size\n",
    "        if self.verbose:\n",
    "            print(f\"Dataset {train_size=}, {val_size=}, and {test_size=}\")\n",
    "        return train_size, val_size, test_size\n",
    "\n",
    "    def _split(self, items):\n",
    "        train_size, val_size, test_size = self._get_split_sizes(len(items))\n",
    "        if self.split_name == \"train\":\n",
    "            # train\n",
    "            self.items = items[:train_size]\n",
    "        elif self.split_name == \"validation\":\n",
    "            # validation\n",
    "            self.items = items[train_size: train_size + val_size]\n",
    "        else:\n",
    "            # test\n",
    "            self.items = items[train_size + val_size:]\n",
    "        if self.ablation == 0:\n",
    "            if len(self.items) % self.batch_size != 0:\n",
    "                warnings.warn(f\"For {self.split_name=} {len(self.items) % self.batch_size} patient(s) MRI(s) \"\n",
    "                              f\"({(len(self.items) % self.batch_size) * OASISTorchDataset.VOLUME_DEPTH}) slices are\"\n",
    "                              f\" not in use\")\n",
    "\n",
    "        return self.items\n",
    "\n",
    "    def _load(self, seed):\n",
    "        items = self._collect_items()\n",
    "        # shuffle\n",
    "        random.seed(seed)\n",
    "        random.shuffle(items)\n",
    "        # ablation\n",
    "        if self.ablation:\n",
    "            items = items[:self.ablation]\n",
    "        # split\n",
    "        self.items = self._split(items)\n",
    "        return self\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.load_volume(*self.items[idx])\n",
    "\n",
    "\n",
    "class OASISTFDataset:\n",
    "    def __init__(self):\n",
    "        self.sample_dataset: OASISTorchDataset = None\n",
    "        self.classnames = None\n",
    "        self.num_classes = None\n",
    "        self.num_items = None\n",
    "        self.recommended_batches = None\n",
    "\n",
    "    @staticmethod\n",
    "    def torch_to_tf(torch_dataset):\n",
    "        def torch_data_gen():\n",
    "            for img, lbl in torch_dataset:\n",
    "                yield tf.convert_to_tensor(img), tf.convert_to_tensor(lbl)\n",
    "\n",
    "        input_shape = (torch_dataset.VOLUME_DEPTH, *torch_dataset.image_size, torch_dataset.nch)\n",
    "        label_shape = (torch_dataset.num_classes,)\n",
    "        input_datatype = (getattr(tf, torch_dataset.dtype), getattr(tf, torch_dataset.dtype))\n",
    "        dataset = tf.data.Dataset.from_generator(torch_data_gen,\n",
    "                                                 output_types=input_datatype,\n",
    "                                                 output_shapes=(input_shape, label_shape)\n",
    "                                                 )\n",
    "        return dataset\n",
    "\n",
    "    def tf_oasis_load_dataset(\n",
    "            self,\n",
    "            directory,\n",
    "            transforms,\n",
    "            label_mode=\"category\",\n",
    "            class_names=None,\n",
    "            color_mode=\"rgb\",\n",
    "            batch_size=32,\n",
    "            ablation=0,\n",
    "            image_size=(256, 256),\n",
    "            seed=None,\n",
    "            split_ratio_100=(70, 20, 10),\n",
    "            dtype=\"float32\",\n",
    "    ):\n",
    "        assert color_mode in (\"grayscale\", \"rgb\"), color_mode\n",
    "        assert label_mode in (OASISTorchDataset.BINARY_TASK, OASISTorchDataset.CATEGORY_TASK), label_mode\n",
    "        assert len(split_ratio_100) == 3 and sum(\n",
    "            split_ratio_100) == 100, f\"train-validation-test splits must accumulate to 100, but {split_ratio_100} given\"\n",
    "        # determine number of channels\n",
    "        nch = 3 if color_mode == \"rgb\" else 1\n",
    "        # load torch dataset\n",
    "        torch_data_train = OASISTorchDataset(directory,\n",
    "                                             task_type=label_mode,\n",
    "                                             image_size=image_size,\n",
    "                                             transforms=transforms,\n",
    "                                             batch_size=batch_size,\n",
    "                                             ablation=ablation,\n",
    "                                             split_name=\"train\",\n",
    "                                             nch=nch,\n",
    "                                             seed=seed,\n",
    "                                             splits=split_ratio_100,\n",
    "                                             class_names=class_names,\n",
    "                                             dtype=dtype,\n",
    "                                             verbose=1)\n",
    "        self.sample_dataset = torch_data_train\n",
    "        torch_data_val = OASISTorchDataset(directory,\n",
    "                                           task_type=label_mode,\n",
    "                                           image_size=image_size,\n",
    "                                           transforms=None,\n",
    "                                           batch_size=batch_size,\n",
    "                                           ablation=ablation,\n",
    "                                           split_name=\"validation\",\n",
    "                                           nch=nch,\n",
    "                                           seed=seed,\n",
    "                                           splits=split_ratio_100,\n",
    "                                           class_names=class_names,\n",
    "                                           dtype=dtype)\n",
    "        torch_data_test = OASISTorchDataset(directory,\n",
    "                                            task_type=label_mode,\n",
    "                                            image_size=image_size,\n",
    "                                            transforms=None,\n",
    "                                            batch_size=batch_size,\n",
    "                                            ablation=ablation,\n",
    "                                            split_name=\"test\",\n",
    "                                            nch=nch,\n",
    "                                            seed=seed,\n",
    "                                            splits=split_ratio_100,\n",
    "                                            class_names=class_names,\n",
    "                                            dtype=dtype)\n",
    "        self.classnames = torch_data_train.classnames\n",
    "        self.num_classes = torch_data_train.num_classes\n",
    "        self.num_items = len(torch_data_train) + len(torch_data_val) + len(torch_data_test)\n",
    "        print(\"Number of patients:\", self.num_items)\n",
    "        print(\"Class Names:\", self.classnames)\n",
    "        print(\"Number of classes:\", self.num_classes)\n",
    "        print(f\"Minimum MRI waste for {batch_size=} is\", self.num_items % batch_size)\n",
    "        print(f\"Minimum MRI slices waste for {batch_size=} is\",\n",
    "              self.num_items % batch_size * torch_data_train.VOLUME_DEPTH)\n",
    "        self.recommended_batches = [i for i in range(2, self.num_items + 1) if self.num_items % i == 0]\n",
    "        print(f\"0 waste batch recommendations:\", self.recommended_batches)\n",
    "        # convert torch to tf dataset\n",
    "        train_dataset = self.torch_to_tf(torch_data_train).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        val_dataset = self.torch_to_tf(torch_data_val).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        test_dataset = self.torch_to_tf(torch_data_test).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        return train_dataset.batch(batch_size), val_dataset.batch(batch_size), test_dataset.batch(batch_size)\n",
    "\n",
    "    def encode_label(self, labels):\n",
    "        return self.sample_dataset.encode_labels(labels)\n",
    "\n",
    "    def decode_labels(self, probabilities: np.ndarray, probability_threshold=0.5):\n",
    "        return self.sample_dataset.decode_labels(probabilities, probability_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading optimization and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " def bilateral_filter(pil_image):\n",
    "        return cv2.bilateralFilter(np.array(pil_image), 15, 75, 75)\n",
    "\n",
    "\n",
    "def img_reshape(tensor):\n",
    "    return tensor.view(image_size, image_size, n_ch)\n",
    "\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Lambda(bilateral_filter),\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(img_reshape),\n",
    "    torchvision.transforms.Normalize(mean=0., std=1.),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">82</span> patients, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5002</span> images scanned from <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/soumensardar/Downloads/OASIS/Mild Dementia'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For \u001b[1;36m82\u001b[0m patients, \u001b[1;36m5002\u001b[0m images scanned from \u001b[32m'/Users/soumensardar/Downloads/OASIS/Mild Dementia'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">225</span> patients, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13725</span> images scanned from <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/soumensardar/Downloads/OASIS/Very mild Dementia'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For \u001b[1;36m225\u001b[0m patients, \u001b[1;36m13725\u001b[0m images scanned from \u001b[32m'/Users/soumensardar/Downloads/OASIS/Very mild Dementia'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> patients, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">488</span> images scanned from <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/soumensardar/Downloads/OASIS/Moderate Dementia'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For \u001b[1;36m8\u001b[0m patients, \u001b[1;36m488\u001b[0m images scanned from \u001b[32m'/Users/soumensardar/Downloads/OASIS/Moderate Dementia'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1102</span> patients, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67222</span> images scanned from <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/soumensardar/Downloads/OASIS/Non Demented'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For \u001b[1;36m1102\u001b[0m patients, \u001b[1;36m67222\u001b[0m images scanned from \u001b[32m'/Users/soumensardar/Downloads/OASIS/Non Demented'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Dataset <span style=\"color: #808000; text-decoration-color: #808000\">train_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">91</span>, <span style=\"color: #808000; text-decoration-color: #808000\">val_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, and <span style=\"color: #808000; text-decoration-color: #808000\">test_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Dataset \u001b[33mtrain_size\u001b[0m=\u001b[1;36m91\u001b[0m, \u001b[33mval_size\u001b[0m=\u001b[1;36m26\u001b[0m, and \u001b[33mtest_size\u001b[0m=\u001b[1;36m13\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Number of patients: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">130</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Number of patients: \u001b[1;36m130\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Class Names:\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Mild Dementia'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Very mild Dementia'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Moderate Dementia'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Non Demented'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Class Names:\n",
       "\u001b[1m[\u001b[0m\u001b[32m'Mild Dementia'\u001b[0m, \u001b[32m'Very mild Dementia'\u001b[0m, \u001b[32m'Moderate Dementia'\u001b[0m, \u001b[32m'Non Demented'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Number of classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Number of classes: \u001b[1;36m4\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Minimum MRI waste for <span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Minimum MRI waste for \u001b[33mbatch_size\u001b[0m=\u001b[1;36m13\u001b[0m is \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Minimum MRI slices waste for <span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Minimum MRI slices waste for \u001b[33mbatch_size\u001b[0m=\u001b[1;36m13\u001b[0m is \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> waste batch recommendations:\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">130</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m waste batch recommendations:\n",
       "\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m13\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m65\u001b[0m, \u001b[1;36m130\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OASISTorchDataset.VOLUME_DEPTH = volume_depth\n",
    "odd = OASISTFDataset()\n",
    "train_ds, val_ds, test_ds = odd.tf_oasis_load_dataset(\n",
    "    datasetdir,\n",
    "    transforms=transforms,\n",
    "    label_mode=task_type,\n",
    "    class_names=None,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    ablation=ablation_study_size,\n",
    "    image_size=(image_size, image_size),\n",
    "    seed=random_seed,\n",
    "    split_ratio_100=(70, 20, 10),\n",
    "    dtype = f\"float{precision}\",\n",
    ")\n",
    "\n",
    "# print(\"train...\")\n",
    "# for i, l in train_ds:\n",
    "#     print(i.shape,  l)\n",
    "#     break\n",
    "\n",
    "# print(\"val...\")\n",
    "# for i, l in val_ds:\n",
    "#     print(i.shape,  l)\n",
    "#     break\n",
    "\n",
    "# print(\"test...\")\n",
    "# for i, l in test_ds:\n",
    "#     print(i.shape,  odd.decode_labels(l))\n",
    "#     break\n",
    "\n",
    "CLASS_NAMES = odd.classnames\n",
    "num_classes = odd.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('class_weights', None, '==============')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = None\n",
    "\"class_weights\", class_weights, \"==============\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Logger\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "from tensorflow.keras.callbacks import LambdaCallback, Callback, LearningRateScheduler\n",
    "import shutil, glob\n",
    "\n",
    "# Create EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", \n",
    "                               mode=\"min\", \n",
    "                               patience=7, \n",
    "                               min_delta=0.001,\n",
    "                               verbose=1,\n",
    "                               start_from_epoch=9,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "csv_logger = CSVLogger(artifact_root + '/metrics.csv')\n",
    "# Tensorboard Logger\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=tf_log_dir,\n",
    "    histogram_freq=0,\n",
    "    write_graph=False,\n",
    "    write_images=False,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None    \n",
    ")\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    artifact_root + \"/models/epoch={epoch:02d}.h5\",\n",
    "    monitor=monitor,\n",
    "    verbose=0,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode=mode,\n",
    "    save_freq=freq,\n",
    "    initial_value_threshold=initial_threshold,\n",
    ")\n",
    "\n",
    "model_best_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    artifact_root + \"/models/best-model.h5\",\n",
    "    monitor=monitor,\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=mode,\n",
    "    save_freq=freq,\n",
    "    initial_value_threshold=initial_threshold,\n",
    ")\n",
    "\n",
    "class BestModelEpochTrackerCallback(Callback):\n",
    "    \"\"\"\n",
    "    This callback monitor best values and updates in a json file project_dir/bestvalue.json\n",
    "    \"\"\"\n",
    "    def __init__(self, monitor, mode, initial_value_threshold=None, verbose=0):\n",
    "        assert mode in (\"min\", \"max\")\n",
    "        initial_thresh = initial_value_threshold\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best_value = initial_thresh\n",
    "        if mode == \"min\":\n",
    "            self.is_better = np.less\n",
    "            if self.best_value is None:\n",
    "                self.best_value = np.Inf\n",
    "        elif mode == \"max\":\n",
    "            self.is_better = np.greater\n",
    "            if self.best_value is None:\n",
    "                self.best_value = -np.Inf \n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def on_epoch_end(self, epoch, metrics=None):\n",
    "        curr_val = metrics.get(self.monitor, None)\n",
    "        assert curr_val is not None, f\"Unable to find the metric to monitor: {self.monitor}\"\n",
    "        if self.is_better(curr_val, self.best_value):\n",
    "            update_path = artifact_root + \"/bestvalue.json\"\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}: {self.monitor} improved form {self.best_value:.5f} to {curr_val:.5f} and saving updates to {update_path}\")\n",
    "            self.best_value = curr_val\n",
    "            with open(update_path, \"w\") as fp:\n",
    "                json.dump(dict(epoch=epoch+1, \n",
    "                               monitor=self.monitor, \n",
    "                               value=curr_val, \n",
    "                               mode=self.mode, \n",
    "                               frequency=\"epoch\"\n",
    "                              ), fp, indent=4)\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                self.print(f\"Epoch {epoch+1}: {self.monitor} did not improved form {self.best_value}\")\n",
    "            \n",
    "                \n",
    "\n",
    "bestval_monitor_callback = BestModelEpochTrackerCallback(\n",
    "    monitor=monitor,\n",
    "    mode=mode,\n",
    "    initial_value_threshold=initial_threshold,\n",
    ")\n",
    "\n",
    "\n",
    "class CleanupCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, metrics=None):\n",
    "        return\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, metrics=None):\n",
    "        # clean last checkpoint assets\n",
    "        last_epoch = epoch - 1\n",
    "        # update current\n",
    "        with open(artifact_root + \"/currentepoch.txt\", \"w\") as fp:\n",
    "            fp.write(f\"{epoch}\")\n",
    "        # look for last epoch checkpoint and delete\n",
    "        pattern = artifact_root + \"/models/epoch=%02d.h5\" % (last_epoch)\n",
    "        if os.path.exists(pattern):\n",
    "            os.remove(pattern)\n",
    "       \n",
    "        \n",
    "        \n",
    "cleanup_callback = CleanupCallback()\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "callbacks = []\n",
    "if lr_schedular:\n",
    "    callbacks.extend([lr_callback])\n",
    "\n",
    "callbacks.extend([csv_logger, tensorboard_callback, model_ckpt, model_best_ckpt, bestval_monitor_callback, cleanup_callback])\n",
    "\n",
    "if early_stop:\n",
    "    callbacks.extend([early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Multiview build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.applications\n",
    "\n",
    "\n",
    "def create_multiview_mobilenet(input_shape, num_views, num_classes, trainable=False):\n",
    "    \"\"\"\n",
    "    Creates a multiview CNN model using MobileNet as the base.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Tuple, shape of each input image (e.g., (224, 224, 3)).\n",
    "        num_views: Integer, number of input views.\n",
    "        num_classes: Integer, number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        A Keras Model instance.\n",
    "    \"\"\"\n",
    "    input_tensor = keras.Input(shape=(num_views, *input_shape), name=\"multi_view_input\")\n",
    "\n",
    "    # Distribute the input to different branches for each view\n",
    "    def distribute_views(views):\n",
    "        unstacked_views = tf.split(views, num_views, axis=1, name=\"view_input_\")\n",
    "        unstacked_views = [tf.squeeze(t, axis=1) for t in unstacked_views]\n",
    "        return unstacked_views\n",
    "\n",
    "    distributed_input = keras.layers.Lambda(distribute_views, name=\"distribute_view_input\")(input_tensor)\n",
    "    # distributed_input = [keras.Input(shape=input_shape) for _ in range(num_views)]\n",
    "\n",
    "    # Create a list to store the outputs of each view's MobileNet\n",
    "    mobilenet_outputs = []\n",
    "\n",
    "    # Create and apply MobileNet to each view\n",
    "    for i in range(num_views):\n",
    "        base_model = keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        setattr(base_model, '_name', f\"{base_model.name}_view_{i}\")\n",
    "        base_model.trainable = trainable  # Freeze base model weights\n",
    "        x = base_model(distributed_input[i])\n",
    "        mobilenet_outputs.append(x)\n",
    "\n",
    "    # Concatenate the outputs of all views\n",
    "    merged_features = keras.layers.Concatenate(axis=-1)(mobilenet_outputs)\n",
    "\n",
    "    # Add custom layers on top of the concatenated features\n",
    "    x = keras.layers.Conv2D(256, (3, 3), activation='relu')(merged_features)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create and compile the final model\n",
    "    model = keras.Model(inputs=input_tensor, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "422-4FV2I3fc",
    "outputId": "302edc0a-3d72-411d-96d3-f491243b826f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Building model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Building model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Building model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Building model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compile model\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compile model\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compile model\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compile model\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " multi_view_input (InputLay  [(None, 31, 128, 128, 3)]    0         []                            \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " distribute_view_input (Lam  [(None, 128, 128, 3),        0         ['multi_view_input[0][0]']    \n",
      " bda)                         (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3),                                                \n",
      "                              (None, 128, 128, 3)]                                                \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_0   (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][0]'\n",
      " (Functional)                                                       ]                             \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_1   (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][1]'\n",
      " (Functional)                                                       ]                             \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_2   (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][2]'\n",
      " (Functional)                                                       ]                             \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_3   (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][3]'\n",
      " (Functional)                                                       ]                             \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_4   (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][4]'\n",
      " (Functional)                                                       ]                             \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_5   (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][5]'\n",
      " (Functional)                                                       ]                             \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_6   (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][6]'\n",
      " (Functional)                                                       ]                             \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_7   (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][7]'\n",
      " (Functional)                                                       ]                             \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_8   (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][8]'\n",
      " (Functional)                                                       ]                             \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_9   (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][9]'\n",
      " (Functional)                                                       ]                             \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_10  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][10]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_11  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][11]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_12  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][12]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_13  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][13]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_14  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][14]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_15  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][15]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_16  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][16]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_17  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][17]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_18  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][18]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_19  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][19]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_20  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][20]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_21  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][21]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_22  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][22]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_23  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][23]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_24  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][24]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_25  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][25]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_26  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][26]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_27  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][27]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_28  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][28]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_29  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][29]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " mobilenet_1.00_128_view_30  (None, 4, 4, 1024)           3228864   ['distribute_view_input[0][30]\n",
      "  (Functional)                                                      ']                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 4, 4, 31744)          0         ['mobilenet_1.00_128_view_0[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'mobilenet_1.00_128_view_1[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'mobilenet_1.00_128_view_2[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'mobilenet_1.00_128_view_3[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'mobilenet_1.00_128_view_4[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'mobilenet_1.00_128_view_5[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'mobilenet_1.00_128_view_6[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'mobilenet_1.00_128_view_7[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'mobilenet_1.00_128_view_8[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'mobilenet_1.00_128_view_9[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'mobilenet_1.00_128_view_10[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_11[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_12[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_13[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_14[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_15[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_16[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_17[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_18[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_19[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_20[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_21[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_22[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_23[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_24[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_25[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_26[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_27[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_28[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_29[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'mobilenet_1.00_128_view_30[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 2, 2, 256)            7313843   ['concatenate[0][0]']         \n",
      "                                                          2                                       \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 1, 1, 256)            0         ['conv2d[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 256)                  0         ['max_pooling2d[0][0]']       \n",
      " GlobalAveragePooling2D)                                                                          \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 256)                  0         ['global_average_pooling2d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 4)                    1028      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 173234244 (660.84 MB)\n",
      "Trainable params: 73139460 (279.00 MB)\n",
      "Non-trainable params: 100094784 (381.83 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture (ResNet50)\n",
    "# import tensorflow.keras as K\n",
    "log(\"Building model...\")\n",
    "import keras\n",
    "mvmodel = create_multiview_mobilenet((image_size, image_size, n_ch), volume_depth, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "log(\"Compile model\")\n",
    "mvmodel.compile(optimizer=optimizer, loss=loss_function, metrics=metrics, run_eagerly=True)\n",
    "mvmodel.summary()\n",
    "model_lenet = mvmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "ngUGLapkJJTS",
    "outputId": "ac1af57f-8bc0-4507-d1ab-60a63d33881b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fresh training<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fresh training\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fresh training<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fresh training\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving hyperparameters.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving hyperparameters.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving hyperparameters.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving hyperparameters.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'timestamp'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'20241223-153109'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'what_changed'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Training with model_name='MultiViewMobileNet' task_type='categorical' lr_schedular=True </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">activation='softmax' loss_function='categorical_crossentropy'\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'random_state'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'max_epoch'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'image_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_channels'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'metrics'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'[&lt;keras.src.metrics.accuracy_metrics.Accuracy object at 0x179318df0&gt;, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">&lt;keras.src.metrics.confusion_metrics.Recall object at 0x3066e6d90&gt;, &lt;keras.src.metrics.confusion_metrics.Precision </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">object at 0x307bac400&gt;, &lt;keras.src.metrics.f_score_metrics.F1Score object at 0x3088720a0&gt;, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">&lt;keras.src.metrics.confusion_metrics.SensitivityAtSpecificity object at 0x308872ac0&gt;, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">&lt;keras.src.metrics.confusion_metrics.SpecificityAtSensitivity object at 0x30888ad30&gt;]'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'loss_function'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'categorical_crossentropy'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'monitoring'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'monitor'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'initial_threshold'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mode'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'max'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'resume_training_timestamp'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'initial_epoch'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'MultiViewMobileNet'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'optimizer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Adam'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'learning_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.001</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'lr_schedular'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'timestamp'\u001b[0m: \u001b[32m'20241223-153109'\u001b[0m,\n",
       "    \u001b[32m'what_changed'\u001b[0m: \u001b[32m\"Training with \u001b[0m\u001b[32mmodel_name\u001b[0m\u001b[32m='MultiViewMobileNet' \u001b[0m\u001b[32mtask_type\u001b[0m\u001b[32m='categorical' \u001b[0m\u001b[32mlr_schedular\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mactivation\u001b[0m\u001b[32m='softmax' \u001b[0m\u001b[32mloss_function\u001b[0m\u001b[32m='categorical_crossentropy'\"\u001b[0m,\n",
       "    \u001b[32m'random_state'\u001b[0m: \u001b[1;36m37\u001b[0m,\n",
       "    \u001b[32m'max_epoch'\u001b[0m: \u001b[1;36m50\u001b[0m,\n",
       "    \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m13\u001b[0m,\n",
       "    \u001b[32m'image_size'\u001b[0m: \u001b[1;36m128\u001b[0m,\n",
       "    \u001b[32m'num_channels'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "    \u001b[32m'metrics'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32m<\u001b[0m\u001b[32mkeras.src.metrics.accuracy_metrics.Accuracy\u001b[0m\u001b[32m object at 0x179318df0>, \u001b[0m\n",
       "\u001b[32m<keras.src.metrics.confusion_metrics.Recall object at 0x3066e6d90>, <keras.src.metrics.confusion_metrics.Precision \u001b[0m\n",
       "\u001b[32mobject at 0x307bac400>, <keras.src.metrics.f_score_metrics.F1Score object at 0x3088720a0>, \u001b[0m\n",
       "\u001b[32m<keras.src.metrics.confusion_metrics.SensitivityAtSpecificity object at 0x308872ac0>, \u001b[0m\n",
       "\u001b[32m<keras.src.metrics.confusion_metrics.SpecificityAtSensitivity object at 0x30888ad30\u001b[0m\u001b[32m>\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'loss_function'\u001b[0m: \u001b[32m'categorical_crossentropy'\u001b[0m,\n",
       "    \u001b[32m'monitoring'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'monitor'\u001b[0m: \u001b[32m'accuracy'\u001b[0m, \u001b[32m'initial_threshold'\u001b[0m: \u001b[1;36m0.5\u001b[0m, \u001b[32m'mode'\u001b[0m: \u001b[32m'max'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'resume_training_timestamp'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'initial_epoch'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "    \u001b[32m'model_name'\u001b[0m: \u001b[32m'MultiViewMobileNet'\u001b[0m,\n",
       "    \u001b[32m'optimizer'\u001b[0m: \u001b[32m'Adam'\u001b[0m,\n",
       "    \u001b[32m'learning_rate'\u001b[0m: \u001b[1;36m0.001\u001b[0m,\n",
       "    \u001b[32m'lr_schedular'\u001b[0m: \u001b[3;92mTrue\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reload checkpoint\n",
    "if resume_checkpoint_path and os.path.exists(resume_checkpoint_path):\n",
    "    log(\"Resuming training...\", resume_checkpoint_path)\n",
    "    model_lenet.load_weights(resume_checkpoint_path)\n",
    "else:\n",
    "    log(\"Fresh training...\")\n",
    "    initial_epoch = 0\n",
    "# daving hyper-params\n",
    "save_hparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Experiment: Started <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">15:31:21</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Experiment: Started \u001b[1;92m15:31:21\u001b[0m \u001b[1;36m23\u001b[0m/\u001b[1;36m12\u001b[0m/\u001b[1;36m2024\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Experiment: Started <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">15:31:21</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Experiment: Started \u001b[1;92m15:31:21\u001b[0m \u001b[1;36m23\u001b[0m/\u001b[1;36m12\u001b[0m/\u001b[1;36m2024\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Starting training <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #800080; text-decoration-color: #800080\">MultiViewMobileNet</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Starting training \u001b[33mmodel\u001b[0m=\u001b[35mMultiViewMobileNet\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Starting training <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #800080; text-decoration-color: #800080\">MultiViewMobileNet</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Starting training \u001b[33mmodel\u001b[0m=\u001b[35mMultiViewMobileNet\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "      7/Unknown - 226s 33s/step - loss: 4336.3506 - accuracy: 0.7308 - recall: 0.6374 - precision: 0.6591 - f1_score: 0.2909 - sensitivity_at_specificity: 0.7473 - specificity_at_sensitivity: 0.9158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soumensardar/miniconda3/envs/tf/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: accuracy improved from 0.50000 to 0.73077, saving model to results/tflogs_mvcnn/MultiViewMobileNet/20241223-153109/models/best-model.h5\n",
      "7/7 [==============================] - 239s 35s/step - loss: 4336.3506 - accuracy: 0.7308 - recall: 0.6374 - precision: 0.6591 - f1_score: 0.2909 - sensitivity_at_specificity: 0.7473 - specificity_at_sensitivity: 0.9158 - val_loss: 3907.8716 - val_accuracy: 0.8654 - val_recall: 0.7308 - val_precision: 0.7308 - val_f1_score: 0.2111 - val_sensitivity_at_specificity: 0.7308 - val_specificity_at_sensitivity: 0.9103 - lr: 0.0200\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - ETA: 0s - loss: 2702.7532 - accuracy: 0.8297 - recall: 0.6593 - precision: 0.6593 - f1_score: 0.2760 - sensitivity_at_specificity: 0.6593 - specificity_at_sensitivity: 0.8864   \n",
      "Epoch 2: accuracy improved from 0.73077 to 0.82967, saving model to results/tflogs_mvcnn/MultiViewMobileNet/20241223-153109/models/best-model.h5\n",
      "7/7 [==============================] - 247s 35s/step - loss: 2702.7532 - accuracy: 0.8297 - recall: 0.6593 - precision: 0.6593 - f1_score: 0.2760 - sensitivity_at_specificity: 0.6593 - specificity_at_sensitivity: 0.8864 - val_loss: 2913.9241 - val_accuracy: 0.8654 - val_recall: 0.7308 - val_precision: 0.7308 - val_f1_score: 0.2111 - val_sensitivity_at_specificity: 0.7308 - val_specificity_at_sensitivity: 0.9103 - lr: 0.0200\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - ETA: 0s - loss: 1773.3455 - accuracy: 0.8571 - recall: 0.7143 - precision: 0.7143 - f1_score: 0.3761 - sensitivity_at_specificity: 0.7143 - specificity_at_sensitivity: 0.9048 \n",
      "Epoch 3: accuracy improved from 0.82967 to 0.85714, saving model to results/tflogs_mvcnn/MultiViewMobileNet/20241223-153109/models/best-model.h5\n",
      "7/7 [==============================] - 218s 30s/step - loss: 1773.3455 - accuracy: 0.8571 - recall: 0.7143 - precision: 0.7143 - f1_score: 0.3761 - sensitivity_at_specificity: 0.7143 - specificity_at_sensitivity: 0.9048 - val_loss: 695.2358 - val_accuracy: 0.7500 - val_recall: 0.5000 - val_precision: 0.5000 - val_f1_score: 0.1979 - val_sensitivity_at_specificity: 0.5000 - val_specificity_at_sensitivity: 0.0000e+00 - lr: 0.0200\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - ETA: 0s - loss: 384.2863 - accuracy: 0.8846 - recall: 0.7802 - precision: 0.7802 - f1_score: 0.3395 - sensitivity_at_specificity: 0.7912 - specificity_at_sensitivity: 0.9304 \n",
      "Epoch 4: accuracy improved from 0.85714 to 0.88462, saving model to results/tflogs_mvcnn/MultiViewMobileNet/20241223-153109/models/best-model.h5\n",
      "7/7 [==============================] - 228s 33s/step - loss: 384.2863 - accuracy: 0.8846 - recall: 0.7802 - precision: 0.7802 - f1_score: 0.3395 - sensitivity_at_specificity: 0.7912 - specificity_at_sensitivity: 0.9304 - val_loss: 632.9934 - val_accuracy: 0.8654 - val_recall: 0.7308 - val_precision: 0.7308 - val_f1_score: 0.2111 - val_sensitivity_at_specificity: 0.7308 - val_specificity_at_sensitivity: 0.9103 - lr: 0.0200\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - ETA: 0s - loss: 205.7644 - accuracy: 0.8846 - recall: 0.7802 - precision: 0.7802 - f1_score: 0.4167 - sensitivity_at_specificity: 0.7912 - specificity_at_sensitivity: 0.9267 \n",
      "Epoch 5: accuracy did not improve from 0.88462\n",
      "7/7 [==============================] - 203s 29s/step - loss: 205.7644 - accuracy: 0.8846 - recall: 0.7802 - precision: 0.7802 - f1_score: 0.4167 - sensitivity_at_specificity: 0.7912 - specificity_at_sensitivity: 0.9267 - val_loss: 139.9490 - val_accuracy: 0.7500 - val_recall: 0.5769 - val_precision: 0.5769 - val_f1_score: 0.2757 - val_sensitivity_at_specificity: 0.7308 - val_specificity_at_sensitivity: 0.8590 - lr: 0.0200\n",
      "Epoch 6/50\n",
      "6/7 [========================>.....] - ETA: 30s - loss: 55.5525 - accuracy: 0.8782 - recall: 0.8205 - precision: 0.8205 - f1_score: 0.4725 - sensitivity_at_specificity: 0.8462 - specificity_at_sensitivity: 0.9402 "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "log(\"Experiment: Started\", time=True)\n",
    "log(f\"Starting training model={model_name}\")\n",
    "history = model_lenet.fit(train_ds, epochs=max_epoch, \n",
    "                          initial_epoch=initial_epoch, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks = callbacks,\n",
    "                          class_weight=class_weights,\n",
    "                          validation_data=val_ds)\n",
    "log(f\"Training done={model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "5wxWcmISJaLB",
    "outputId": "4c5daded-5a68-4990-c8d2-bc1b4ba85d89"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "log(\"Experiment: Evaluating\", time=True)\n",
    "log(f\"Evaluating model={model_name}...\")\n",
    "test_result = model_lenet.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='loss', **kwargs):\n",
    "        super().__init__(name='loss', **kwargs)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        pass\n",
    "\n",
    "    def result(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "sYKD-JQrJaOn",
    "outputId": "780d0265-f3b2-43bf-9cf6-de99785400ed"
   },
   "outputs": [],
   "source": [
    "metrics.insert(0, LossMetric())\n",
    "\n",
    "log(f\"{model_name}\", {f\"test_{k.name}\":v for v,k in zip(test_result, metrics)})\n",
    "\n",
    "# TF-logging test values\n",
    "file_writer = tf.summary.create_file_writer(tf_log_dir + \"/test\")\n",
    "with file_writer.as_default():\n",
    "    for v, k in zip(test_result, metrics):\n",
    "        tf.summary.scalar(f\"test_{k.name}\", v, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for imgs, lbls in val_ds:\n",
    "    proba_lbls = model_lenet.predict(imgs)\n",
    "    actual = odd.decode_labels(lbls.numpy())\n",
    "    predicted = odd.decode_labels(proba_lbls)\n",
    "    print(np.array([actual, predicted]).T)\n",
    "    \n",
    "for imgs, lbls in test_ds:\n",
    "    proba_lbls = model_lenet.predict(imgs)\n",
    "    \n",
    "    actual = odd.decode_labels(lbls.numpy())\n",
    "    predicted = odd.decode_labels(proba_lbls)\n",
    "    print(np.array([actual, predicted]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "Y_AmXWIKJpgw",
    "outputId": "406d1461-664c-4991-a208-dbebad70e3b3"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def preprocess_image(image_path):\n",
    "    # Open the image file\n",
    "    img_array, lbl, cls_name = odd.sample_dataset.get_volume_for_image(image_path)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    print(img_array.shape)\n",
    "    return img_array\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "actual_class = TEST_IMG_LABEL\n",
    "image_path = TEST_IMG_PATH\n",
    "image_array = preprocess_image(image_path)\n",
    "log(\"Shape of preprocessed image array:\", image_array.shape)\n",
    "\n",
    "# Predict probabilities\n",
    "log(\"Experiment: Testing\", time=True)\n",
    "prediction_probabilities = model_lenet.predict(image_array)\n",
    "\n",
    "# Get the index of the highest probability\n",
    "predicted_class_index = np.argmax(prediction_probabilities)\n",
    "\n",
    "# Define your class labelsa\n",
    "class_labels = CLASS_NAMES\n",
    "\n",
    "# Map the index to the corresponding class label\n",
    "predicted_class = class_labels[predicted_class_index]\n",
    "\n",
    "# TF-logging test values\n",
    "file_writer = tf.summary.create_file_writer(tf_log_dir + \"/test\")\n",
    "with file_writer.as_default():\n",
    "    tf.summary.text(f\"Corner Case\", f\"Actual={actual_class} Predicted={predicted_class}\", step=0)\n",
    "\n",
    "log(\"Actual class:\", actual_class)\n",
    "log(\"Predicted class:\", predicted_class)\n",
    "log(\"Experiment: Completed\", time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(f\"Your result is saved at: '{artifact_root}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
